Любой текстовый символ можно обозначить числом и соответственно каждый код означает свой символ.

Каждое символ в компьютере представляется в виде целого числа, этим числом называется кодом символа. Коды символов стандартизированы.

Стандарт для английского языка ASCII. 7 бит на каждйы ASCII символ для чисел от 0 до 127. И всего 128 символов получается.

В памяти ячейка на 8 бит но стандарт 7 битный и первый бит всегда будет пустой.

Первые 31 управляющие (LF, CR…)

Последующие имеют внешний вид

Кодовая страница - набор из 256 символов для конкретного языка.

первые 128 совпадают с ASCII. остальные 128 для конкретного языка.

ASNI расширенный стандарт ASCI от Windows. Для ASNI стали создоваться кодовые странцы вида windows-125X, где X - номер страны. Так например windows-1251 - кириллица.

UNICODE - международный стандарт всех симоволов. Каждый символ представлялся минимум 2 байтами а максимум неограничен. U+(код символа - формат записи. Первые 128 символов такие же как в ASCII.

Проблема UNICODE использование 2 байтов памяти. В зависимости от режима работы байты могли читаться по-разному(big endian or little endian). Чтобы решить пробелму хранения строк в памяти появляются кодировки.

Кодировки - правила описывающие хранение UNICODE символов в памяти.

Первая кодировка UCS-2. Перед отправкой или сохранением файла в самое начало добовлялось 2 байтовое число BOM-байты(unicode byte order mark) которое считывалось и было понятно в какой последовательности считывать символы. Получилось BOM-байты + символы по 2 байта.

UTF-8 (United transformation format, 8 потому что минимальное количесвто бит на символ). Было решено исопльзовать символы динамичского размера от 1 байта до 4.

Первые 128 символов помещается в 8 бит. от 0 до 7. Сначала 0 потом номер символа. Далее по аналогии. Вначале идут метки для понимания как считывать символы.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/bc93acf9-0c65-448b-8aee-172ff6347445/899801c4-6be7-462e-9cfa-409a26abeba1/Untitled.png)

Пример на букве П

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/bc93acf9-0c65-448b-8aee-172ff6347445/6e1682fe-5453-4692-82e7-096158cb0d5b/Untitled.png)

Проблема кодировки возникла из-за больших затрат времени.

UTF-16. Совместимость со страой кодировки + хранить больше чем 2 байта. Любой символ теперь занимает 16 бит а дальше расширятеся до 4 бит и называется суррогатной парой.

Алгорит кодирования. Коды символа, которые хранятся используют 4 байта, хранятся в диапозоне в 16 чистеме исчесления. Все коды которые меньше 10000 хранятся используя 2 байта. В этьом диапозоне есть неиспользуемый диапозон (для кодирования суррогатов) - D800 до DFFF.

Пример для символа из UNICODE U+10459 размеров 3 байта.

Из кода вычитается 10000

10459-10000=459. Получается длина от 0 до 16 бит. К получившемуся числу в старшие биты приписываем 0 чтобы получилась длина 20 бит. Делим число пополам и к первой половние прибавляем D800, ко второй DC00. Оба эти числа взяты из кодирования для суррогатов. Результатом 2 числа которые попадают в этот диапозон. D800…DBFF и ВС00…DFFF. Переведя код числа в 16 -ричное представление получается код числа D8+01+DC+59

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/bc93acf9-0c65-448b-8aee-172ff6347445/35a1e63b-8b9a-4969-8ff3-5646a7a9cd57/Untitled.png)

Работает быстрее чем UTF-8 потому что не нужно бегать по памяти и искать начало строки.

Проблема UTF-16 - символы за пределом 2^16 требуют для хранения 4 байта. Используется в C#, java, javascript

UTF-32. Фиксированный размер символа размером в 4 байта для быстрого разбиения строк по 4 байта и двигаться по ним. Так же используются BOM-байты. Используется в Python.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/bc93acf9-0c65-448b-8aee-172ff6347445/6e1b9553-3795-4705-8ba2-c0c283de7a1f/Untitled.png)

От языка зависит используется разная кодировка.